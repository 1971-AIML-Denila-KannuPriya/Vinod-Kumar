{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "792da3e9-d0ff-4237-a9e1-61551ca4ab4e",
   "metadata": {},
   "source": [
    "1. Problem: Implement the K-nearest neighbors (KNN) algorithm from scratch. Given a dataset and a query point, classify the query based on the majority label of its k-nearest neighbors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1fa105c0-3e38-4418-aa1d-fdce4fcf53f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "class KNN:\n",
    "    def __init__(self, k=3):\n",
    "        self.k = k\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.X_train = X\n",
    "        self.y_train = y\n",
    "\n",
    "    def predict(self, X):\n",
    "        predictions = [self._predict(x) for x in X]\n",
    "        return np.array(predictions)\n",
    "\n",
    "    def _predict(self, x):\n",
    "        distances = np.sqrt(np.sum((self.X_train - x) ** 2, axis=1))\n",
    "        k_indices = np.argsort(distances)[:self.k]\n",
    "        k_nearest_labels = [self.y_train[i] for i in k_indices]\n",
    "        most_common = Counter(k_nearest_labels).most_common(1)\n",
    "        return most_common[0][0]\n",
    "\n",
    "X_train = np.array([[1, 2], [2, 3], [3, 4], [5, 6]])\n",
    "y_train = np.array([0, 0, 1, 1])\n",
    "knn = KNN(k=3)\n",
    "knn.fit(X_train, y_train)\n",
    "print(knn.predict(np.array([[3, 3], [6, 5]])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b57111-6412-48c3-9b2c-286a825517e2",
   "metadata": {},
   "source": [
    "2. Problem: Implement logistic regression in Python. Include training using gradient descent, and calculate class probabilities for binary classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b715e392-ab07-49f8-9c9d-bad0c63e79f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1]\n"
     ]
    }
   ],
   "source": [
    "class LogisticRegression:\n",
    "    def __init__(self, learning_rate=0.01, n_iterations=1000):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_iterations = n_iterations\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        self.weights = np.zeros(n_features)\n",
    "        self.bias = 0\n",
    "\n",
    "        for _ in range(self.n_iterations):\n",
    "            linear_model = np.dot(X, self.weights) + self.bias\n",
    "            y_predicted = self.sigmoid(linear_model)\n",
    "\n",
    "            dw = (1 / n_samples) * np.dot(X.T, (y_predicted - y))\n",
    "            db = (1 / n_samples) * np.sum(y_predicted - y)\n",
    "\n",
    "            self.weights -= self.learning_rate * dw\n",
    "            self.bias -= self.learning_rate * db\n",
    "\n",
    "    def predict(self, X):\n",
    "        linear_model = np.dot(X, self.weights) + self.bias\n",
    "        y_predicted = self.sigmoid(linear_model)\n",
    "        class_labels = [1 if i > 0.5 else 0 for i in y_predicted]\n",
    "        return np.array(class_labels)\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "X_train = np.array([[0, 0], [1, 1], [1, 0], [0, 1]])\n",
    "y_train = np.array([0, 1, 1, 0])\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "print(model.predict(np.array([[0.5, 0.5], [1, 1]])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d2f913b-4742-4324-95fc-cf865517eb5f",
   "metadata": {},
   "source": [
    "3. Problem: Write a function to perform matrix multiplication without using external libraries like NumPy. Multiply two matrices representing weight matrices and input vectors in a neural network layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d1efe6c-998f-4335-bcf3-abee64655434",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[19, 22], [43, 50]]\n"
     ]
    }
   ],
   "source": [
    "def matrix_multiply(A, B):\n",
    "    # Check if A's columns match B's rows\n",
    "    if len(A[0]) != len(B):\n",
    "        raise ValueError(\"Cannot multiply, incompatible dimensions.\")\n",
    "    \n",
    "    result = [[0] * len(B[0]) for _ in range(len(A))]\n",
    "    \n",
    "    for i in range(len(A)):\n",
    "        for j in range(len(B[0])):\n",
    "            for k in range(len(B)):\n",
    "                result[i][j] += A[i][k] * B[k][j]\n",
    "    \n",
    "    return result\n",
    "A = [[1, 2], [3, 4]]\n",
    "B = [[5, 6], [7, 8]]\n",
    "print(matrix_multiply(A, B))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e59850c-9ec1-4eec-8867-17737e8ffee9",
   "metadata": {},
   "source": [
    "4. Problem: Write a Python function that deep copies a neural network represented as a\n",
    "nested list of layers, where each layer contains weight matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f4f413ee-aeec-4a59-8de8-deddb34acfb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 2], [3, 4]]\n"
     ]
    }
   ],
   "source": [
    "def deep_copy_neural_network(network):\n",
    "    return [layer.copy() for layer in network]\n",
    "network = [[1, 2], [3, 4]]\n",
    "new_network = deep_copy_neural_network(network)\n",
    "print(new_network)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac76ba6-dc45-4039-a8bd-2d7546343df8",
   "metadata": {},
   "source": [
    "5. Problem: Implement a moving average filter for a time series. Given a series of numbers\n",
    "and a window size, return the moving average of the series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "310ed8f2-7fa0-425d-aabd-8bbe7fb9ffce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.0, 3.0, 4.0]\n"
     ]
    }
   ],
   "source": [
    "def moving_average(data, window_size):\n",
    "    if window_size < 1:\n",
    "        raise ValueError(\"Window size must be at least 1\")\n",
    "    \n",
    "    averages = []\n",
    "    for i in range(len(data) - window_size + 1):\n",
    "        window = data[i:i + window_size]\n",
    "        averages.append(sum(window) / window_size)\n",
    "    \n",
    "    return averages\n",
    "    \n",
    "data = [1, 2, 3, 4, 5]\n",
    "print(moving_average(data, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "052c8d36-f242-454a-a081-c3dc565bb92b",
   "metadata": {},
   "source": [
    "6. Problem: Write a Python function that applies the ReLU activation function to a list of\n",
    "inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "989e9d42-b4d7-42e5-84c9-abf32494331d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 2, 0, 4]\n"
     ]
    }
   ],
   "source": [
    "def relu(inputs):\n",
    "    return [max(0, x) for x in inputs]\n",
    "inputs = [-1, 2, -3, 4]\n",
    "print(relu(inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8c0f0e-6ef0-453e-bb38-f5cad9dbd0f9",
   "metadata": {},
   "source": [
    "7. Problem: Implement gradient descent to train a linear regression model. Minimize the\n",
    "mean squared error by adjusting the model weights iteratively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "40f93707-7a48-4046-94e2-202a8521d10b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.95494716]\n"
     ]
    }
   ],
   "source": [
    "class LinearRegression:\n",
    "    def __init__(self, learning_rate=0.01, n_iterations=1000):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_iterations = n_iterations\n",
    "        self.weights = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        self.weights = np.zeros(n_features)\n",
    "\n",
    "        for _ in range(self.n_iterations):\n",
    "            y_predicted = np.dot(X, self.weights)\n",
    "            error = y_predicted - y\n",
    "            gradients = (2 / n_samples) * np.dot(X.T, error)\n",
    "            self.weights -= self.learning_rate * gradients\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.dot(X, self.weights)\n",
    "\n",
    "X_train = np.array([[1, 1], [1, 2], [2, 2], [2, 3]])\n",
    "y_train = np.array([1, 2, 2, 3])\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "print(model.predict(np.array([[3, 5]])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5772c357-f4eb-4fe5-b3e0-17eb3f400e3c",
   "metadata": {},
   "source": [
    "8. Problem: Write a Python function to perform one-hot encoding of categorical labels for a machine learning task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a49e5f25-7e37-461c-9d3b-f3a40bc1d108",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "def one_hot_encode(labels):\n",
    "    unique_labels = list(set(labels))\n",
    "    encoding = {label: idx for idx, label in enumerate(unique_labels)}\n",
    "    one_hot = np.zeros((len(labels), len(unique_labels)))\n",
    "\n",
    "    for i, label in enumerate(labels):\n",
    "        one_hot[i][encoding[label]] = 1\n",
    "\n",
    "    return one_hot\n",
    "labels = ['cat', 'dog', 'cat', 'bird']\n",
    "print(one_hot_encode(labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b28603-fc4b-4e02-963b-4212249a701c",
   "metadata": {},
   "source": [
    "9. Problem: Implement a Python function to calculate the cosine similarity between two\n",
    "vectors, which is often used in text similarity tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "577fd169-15e4-4a47-8d36-759de98eb0e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9746318461970762\n"
     ]
    }
   ],
   "source": [
    "def cosine_similarity(vec1, vec2):\n",
    "    dot_product = sum(a * b for a, b in zip(vec1, vec2))\n",
    "    norm_a = sum(a ** 2 for a in vec1) ** 0.5\n",
    "    norm_b = sum(b ** 2 for b in vec2) ** 0.5\n",
    "    return dot_product / (norm_a * norm_b) if norm_a and norm_b else 0.0\n",
    "    \n",
    "vec1 = [1, 2, 3]\n",
    "vec2 = [4, 5, 6]\n",
    "print(cosine_similarity(vec1, vec2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ee86b7-1042-4e4c-af97-1df002533a62",
   "metadata": {},
   "source": [
    "10. Problem: Given a trained neural network in Python, write a function to prune the network by removing neurons that have small or zero weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1ccf444b-6417-404b-8548-7b05c48669c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0.1, 0.2], [0, 0.3]], [[0.4, 0], [0.5, 0.6]]]\n"
     ]
    }
   ],
   "source": [
    "def prune_neurons(network, threshold=0.01):\n",
    "    pruned_network = []\n",
    "    for layer in network:\n",
    "        pruned_layer = [[weight if abs(weight) > threshold else 0 for weight in neuron] for neuron in layer]\n",
    "        pruned_network.append(pruned_layer)\n",
    "    return pruned_network\n",
    "\n",
    "network = [[[0.1, 0.2], [0.0, 0.3]], [[0.4, 0.0], [0.5, 0.6]]]\n",
    "print(prune_neurons(network))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b99ba76-fd98-454c-be38-9a554a7f98d3",
   "metadata": {},
   "source": [
    "11. Problem: Implement a Python function to compute the confusion matrix given true and\n",
    "predicted labels for a classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cc6ac701-1e94-4ee5-8dcc-17433a286e31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1]\n",
      " [1 2]]\n"
     ]
    }
   ],
   "source": [
    "def confusion_matrix(true_labels, predicted_labels):\n",
    "    classes = list(set(true_labels))\n",
    "    matrix = np.zeros((len(classes), len(classes)), dtype=int)\n",
    "\n",
    "    for true, pred in zip(true_labels, predicted_labels):\n",
    "        matrix[classes.index(true)][classes.index(pred)] += 1\n",
    "\n",
    "    return matrix\n",
    "\n",
    "true_labels = [0, 1, 0, 1, 1]\n",
    "predicted_labels = [0, 0, 1, 1, 1]\n",
    "print(confusion_matrix(true_labels, predicted_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186120b2-9142-46c2-93af-1e7b5f6dcf5c",
   "metadata": {},
   "source": [
    "12. Problem: Write a Python function that performs mini-batch gradient descent for optimizing the weights of a neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6e68e38a-3211-47e2-a216-39967d85e263",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.99428493]\n"
     ]
    }
   ],
   "source": [
    "class MiniBatchGradientDescent:\n",
    "    def __init__(self, learning_rate=0.01, n_iterations=1000, batch_size=32):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_iterations = n_iterations\n",
    "        self.batch_size = batch_size\n",
    "        self.weights = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        self.weights = np.zeros(n_features)\n",
    "\n",
    "        for _ in range(self.n_iterations):\n",
    "            indices = np.random.permutation(n_samples)\n",
    "            X_shuffled = X[indices]\n",
    "            y_shuffled = y[indices]\n",
    "\n",
    "            for i in range(0, n_samples, self.batch_size):\n",
    "                X_batch = X_shuffled[i:i + self.batch_size]\n",
    "                y_batch = y_shuffled[i:i + self.batch_size]\n",
    "\n",
    "                y_predicted = np.dot(X_batch, self.weights)\n",
    "                gradients = (2 / self.batch_size) * np.dot(X_batch.T, (y_predicted - y_batch))\n",
    "                self.weights -= self.learning_rate * gradients\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.dot(X, self.weights)\n",
    "\n",
    "X_train = np.array([[1, 1], [1, 2], [2, 2], [2, 3]])\n",
    "y_train = np.array([1, 2, 2, 3])\n",
    "model = MiniBatchGradientDescent(batch_size=2)\n",
    "model.fit(X_train, y_train)\n",
    "print(model.predict(np.array([[3, 5]])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "326823b8-6224-4a9d-9515-572594de9afd",
   "metadata": {},
   "source": [
    "13. Problem: Implement k-means clustering from scratch. Given a dataset and the number of\n",
    "clusters k , return the cluster assignments for each data point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "05e7f642-2b38-49a2-a73b-bd57d88b49dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "class KMeans:\n",
    "    def __init__(self, k=3, max_iters=100):\n",
    "        self.k = k\n",
    "        self.max_iters = max_iters\n",
    "\n",
    "    def fit(self, X):\n",
    "        n_samples, n_features = X.shape\n",
    "        random_indices = np.random.choice(n_samples, self.k, replace=False)\n",
    "        self.centroids = X[random_indices]\n",
    "\n",
    "        for _ in range(self.max_iters):\n",
    "            labels = self._get_labels(X)\n",
    "            self.centroids = self._get_centroids(X, labels)\n",
    "\n",
    "    def _get_labels(self, X):\n",
    "        distances = np.linalg.norm(X[:, np.newaxis] - self.centroids, axis=2)\n",
    "        return np.argmin(distances, axis=1)\n",
    "\n",
    "    def _get_centroids(self, X, labels):\n",
    "        return np.array([X[labels == i].mean(axis=0) for i in range(self.k)])\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self._get_labels(X)\n",
    "\n",
    "X_train = np.array([[1, 2], [1, 4], [1, 0], [4, 2], [4, 4], [4, 0]])\n",
    "kmeans = KMeans(k=2)\n",
    "kmeans.fit(X_train)\n",
    "print(kmeans.predict(X_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d0257c-aba3-42e6-a7dd-fa4c74149960",
   "metadata": {},
   "source": [
    "14. Problem: Implement a Python function to calculate the softmax of a list of numbers, which is used in multi-class classification problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1cb6c026-cb7a-418c-b753-f017b15dc2eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.09003057 0.24472847 0.66524096]\n"
     ]
    }
   ],
   "source": [
    "def softmax(x):\n",
    "    exp_x = np.exp(x - np.max(x))\n",
    "    return exp_x / exp_x.sum(axis=0)\n",
    "scores = [1.0, 2.0, 3.0]\n",
    "print(softmax(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ef1766-55eb-4b86-898d-7d451d98f6ac",
   "metadata": {},
   "source": [
    "15. Problem: Write a Python function to compute the TF-IDF (Term Frequency-Inverse\n",
    "Document Frequency) for a list of documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "adcbf7c2-003e-45e8-a251-8e24886ff08e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'Hi': 0.0, 'my': 0.0, 'name': 0.0, 'is': 0.0, 'Fayaz': 0.13862943611198905}, {'Hi': 0.0, 'my': 0.0, 'name': 0.0, 'is': 0.0, 'Abdul': 0.13862943611198905}]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import math\n",
    "\n",
    "def tf_idf(documents):\n",
    "    tf = []\n",
    "    idf = {}\n",
    "    total_documents = len(documents)\n",
    "\n",
    "    for doc in documents:\n",
    "        counter = Counter(doc)\n",
    "        tf.append({word: count / len(doc) for word, count in counter.items()})\n",
    "\n",
    "    for doc in documents:\n",
    "        for word in set(doc):\n",
    "            idf[word] = idf.get(word, 0) + 1\n",
    "\n",
    "    idf = {word: math.log(total_documents / count) for word, count in idf.items()}\n",
    "\n",
    "    tf_idf_result = []\n",
    "    for doc_tf in tf:\n",
    "        doc_result = {word: doc_tf[word] * idf[word] for word in doc_tf}\n",
    "        tf_idf_result.append(doc_result)\n",
    "\n",
    "    return tf_idf_result\n",
    "\n",
    "documents = [[\"Hi\", \"my\", \"name\", \"is\", \"Fayaz\"], [\"Hi\", \"my\", \"name\", \"is\", \"Abdul\"]]\n",
    "print(tf_idf(documents))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c197160-88d9-4b47-8797-3afdf591133d",
   "metadata": {},
   "source": [
    "16. Problem: Implement an algorithm to find the principal components of a dataset using\n",
    "singular value decomposition (SVD), which is a core part of PCA (Principal Component\n",
    "Analysis)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "75798413-19aa-4372-8659-d1077b626e4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.41421356]\n",
      " [ 0.        ]\n",
      " [ 1.41421356]]\n"
     ]
    }
   ],
   "source": [
    "def pca(X, n_components):\n",
    "    X_meaned = X - np.mean(X, axis=0)\n",
    "    U, S, Vt = np.linalg.svd(X_meaned)\n",
    "    W = Vt.T[:, :n_components]\n",
    "    return X_meaned @ W\n",
    "\n",
    "X = np.array([[2, 3], [3, 4], [4, 5]])\n",
    "print(pca(X, n_components=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "637fd87a-7bf4-4693-94df-f683961cf165",
   "metadata": {},
   "source": [
    "17. Problem: Write a Python function to calculate the AUC-ROC score for a binary\n",
    "classification problem, given the true labels and predicted probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cefaf2f1-aeaf-4913-bc92-76b25dfe1a2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.75\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "def calculate_auc_roc(y_true, y_scores):\n",
    "    return roc_auc_score(y_true, y_scores)\n",
    "y_true = [0, 0, 1, 1]\n",
    "y_scores = [0.1, 0.4, 0.35, 0.8]\n",
    "print(calculate_auc_roc(y_true, y_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfda84dc-ff8d-434f-a42d-340d74c036cd",
   "metadata": {},
   "source": [
    "18. Problem: Implement a Python function to apply dropout regularization to the neurons of a given neural network during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "799ab6d4-288f-4e6f-a6d6-c1c9c954bd7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0.9]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def dropout(layer, rate):\n",
    "    return [neuron if random.random() > rate else 0 for neuron in layer]\n",
    "\n",
    "layer = [0.5, 0.3, 0.9]\n",
    "print(dropout(layer, rate=0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c69079-7125-42c3-9a75-13d78e6b6f46",
   "metadata": {},
   "source": [
    "19. Problem: Write a Python function to perform feature scaling using standardization (z-score normalization), which transforms features to have a mean of 0 and a standard deviation of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "616603dd-0133-4302-84ea-2a1d865a172a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.22474487 -1.22474487]\n",
      " [ 0.          0.        ]\n",
      " [ 1.22474487  1.22474487]]\n"
     ]
    }
   ],
   "source": [
    "def standardize(X):\n",
    "    mean = np.mean(X, axis=0)\n",
    "    std_dev = np.std(X, axis=0)\n",
    "    return (X - mean) / std_dev\n",
    "\n",
    "X = np.array([[1, 2], [3, 4], [5, 6]])\n",
    "print(standardize(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14bc0f75-da24-4048-bce6-f0f1b0ad7778",
   "metadata": {},
   "source": [
    "20. Problem: Implement a Python function to compute the cross-entropy loss for a multi-class classification problem, given the true labels and predicted probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "737fb79f-02ba-4ef3-b7be-5a7b50f89d6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.10950135565734533\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "def cross_entropy_loss(y_true, y_pred):\n",
    "    epsilon = 1e-12  \n",
    "    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)  # Prevent NaNs\n",
    "    return -np.mean(y_true * np.log(y_pred))\n",
    "\n",
    "y_true = np.array([1, 0, 1])\n",
    "y_pred = np.array([0.9, 0.1, 0.8])\n",
    "print(cross_entropy_loss(y_true, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
